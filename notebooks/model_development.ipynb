{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a47feeb5-19c9-4469-93ce-facc16b86579",
     "showTitle": false,
     "title": ""
    },
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Bryan-Az/YumTum/blob/main/Restaurant_Recommender.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bcc06c4-c735-46e3-8f38-8b8a98e261e5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Loading the Datasets using SQL Warehouse and Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f85099f6-cb0b-4e97-af70-bfd8b8dad822",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0b372d0-bd0f-4119-87b0-77747f487716",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# reading in test as a pd df\n",
    "test_full = ps.read_table('test_full')\n",
    "test_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc15d595-b0dd-4551-af23-15e157200fd7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# reading in train as a pd df\n",
    "train_full = ps.read_table('train_full')\n",
    "train_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1e0a366-7111-448c-9ea1-b4e59acd563f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Processing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d306f0a-df61-44e1-b49a-dffe8113145e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_full.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "066b6aa0-40de-4351-b765-281f891c7177",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "type(test_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84abe189-007b-44e2-9667-31e838050f3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Sampling\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import lit, concat_ws, col\n",
    "seed=1\n",
    "# Define a comprehensive set of stratifying variables\n",
    "stratifying_vars = ['gender', 'language', 'location_type']\n",
    "\n",
    "# Function to perform stratified sampling\n",
    "def stratified_sample(df, stratify_cols, frac=0.25):\n",
    "    df = df.to_spark()\n",
    "    # Concatenate the stratifying columns to create a composite key\n",
    "    df = df.withColumn('composite_key', concat_ws('_', *stratify_cols))\n",
    "\n",
    "    # Generate fractions for each unique combination of the stratifying columns\n",
    "    fractions = (\n",
    "        df.select('composite_key')\n",
    "          .distinct()\n",
    "          .withColumn(\"fraction\", lit(frac))\n",
    "          .rdd\n",
    "          .map(lambda row: (row['composite_key'], row['fraction']))\n",
    "          .collectAsMap()\n",
    "    )\n",
    "\n",
    "    # Use the composite key for stratified sampling\n",
    "    sampled_df = df.stat.sampleBy('composite_key', fractions, seed)\n",
    "    return sampled_df\n",
    "\n",
    "# Applying stratified sampling on train and test datasets\n",
    "train_sampled = stratified_sample(train_full, stratifying_vars, frac=0.25)\n",
    "test_sampled = stratified_sample(test_full, stratifying_vars, frac=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fed7d29-1352-428c-bfd9-18af4d4aa15d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_sampled_pd = train_sampled.to_pandas_on_spark()\n",
    "train_sampled_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a88d2efa-caa1-4b67-8c84-63226c235e6e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_sampled_pd = test_sampled.to_pandas_on_spark()\n",
    "train_sampled_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "600e094c-e349-424c-979d-5e00278db92c",
     "showTitle": false,
     "title": ""
    },
    "id": "6e05a3ad"
   },
   "outputs": [],
   "source": [
    "#Feature reduction\n",
    "# Updated columns to keep for the training dataset\n",
    "train_columns_to_keep = [\n",
    "    'customer_id', 'gender', 'language',\n",
    "    'location_number', 'location_type', 'latitude_x', 'longitude_x',\n",
    "    'id', 'latitude_y', 'longitude_y', 'vendor_category_en', 'vendor_rating',\n",
    "    'delivery_charge', 'serving_distance', 'is_open', 'discount_percentage',\n",
    "    'display_orders',\n",
    "    'vendor_tag_name','sunday_from_time1', 'sunday_to_time1', 'sunday_from_time2',\n",
    "    'sunday_to_time2', 'monday_from_time1', 'monday_to_time1', 'monday_from_time2',\n",
    "    'monday_to_time2', 'tuesday_from_time1', 'tuesday_to_time1', 'tuesday_from_time2',\n",
    "    'tuesday_to_time2', 'wednesday_from_time1', 'wednesday_to_time1', 'wednesday_from_time2',\n",
    "    'wednesday_to_time2', 'thursday_from_time1', 'thursday_to_time1', 'thursday_from_time2',\n",
    "    'thursday_to_time2', 'friday_from_time1', 'friday_to_time1', 'friday_from_time2',\n",
    "    'friday_to_time2', 'saturday_from_time1', 'saturday_to_time1', 'saturday_from_time2',\n",
    "    'saturday_to_time2',\n",
    "    'target'  # 'target' column is specific to the training dataset\n",
    "]\n",
    "\n",
    "# Updated columns to keep for the testing dataset\n",
    "test_columns_to_keep = [\n",
    "    'customer_id', 'gender', 'language',\n",
    "    'location_number', 'location_type', 'latitude_x', 'longitude_x',\n",
    "    'id', 'latitude_y', 'longitude_y', 'vendor_category_en', 'vendor_rating',\n",
    "    'delivery_charge', 'serving_distance', 'is_open', 'discount_percentage',\n",
    "    'display_orders',  # Including 'display_orders'\n",
    "    'vendor_tag_name','sunday_from_time1', 'sunday_to_time1', 'sunday_from_time2',\n",
    "    'sunday_to_time2', 'monday_from_time1', 'monday_to_time1', 'monday_from_time2',\n",
    "    'monday_to_time2', 'tuesday_from_time1', 'tuesday_to_time1', 'tuesday_from_time2',\n",
    "    'tuesday_to_time2', 'wednesday_from_time1', 'wednesday_to_time1', 'wednesday_from_time2',\n",
    "    'wednesday_to_time2', 'thursday_from_time1', 'thursday_to_time1', 'thursday_from_time2',\n",
    "    'thursday_to_time2', 'friday_from_time1', 'friday_to_time1', 'friday_from_time2', 'friday_to_time2',\n",
    "    'saturday_from_time1', 'saturday_to_time1', 'saturday_from_time2', 'saturday_to_time2'\n",
    "]\n",
    "\n",
    "\n",
    "# Reduce features in the training dataset\n",
    "train_reduced = train_sampled_pd[train_columns_to_keep]\n",
    "\n",
    "# Reduce features in the testing dataset\n",
    "test_reduced = test_sampled_pd[test_columns_to_keep]\n",
    "\n",
    "# Save the reduced datasets (dbfs)\n",
    "train_re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dc60b71-1c24-4587-8acf-dadaa70ae39f",
     "showTitle": false,
     "title": ""
    },
    "id": "4a2d3c9d",
    "outputId": "30ec61c9-4b27-45a9-bb0d-4dbac49f5a8d"
   },
   "outputs": [],
   "source": [
    "train_reduced.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "083173fd-00a5-4228-9f02-cd18dfcd3ea9",
     "showTitle": false,
     "title": ""
    },
    "id": "c73c234f",
    "outputId": "b4cb21bb-a8d7-4538-907c-4ffd6cf441fa"
   },
   "outputs": [],
   "source": [
    "test_reduced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4cc009a-0b2d-4a54-8e84-3e3287d0a424",
     "showTitle": false,
     "title": ""
    },
    "id": "360ab9d2",
    "outputId": "ab2d05d9-a6f0-4b52-b29e-2ba8c00cfe1d"
   },
   "outputs": [],
   "source": [
    "len(train_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afe6e007-0cc7-47b0-89ec-da335a8141c6",
     "showTitle": false,
     "title": ""
    },
    "id": "25bc5697",
    "outputId": "d8923510-260c-469b-9522-e96c7319bd45"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "train_data = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/train_reduced_cleaned.csv\")\n",
    "\n",
    "# Display a sample of the specific columns\n",
    "sample_data = train_data[['vendor_category_en', 'vendor_tag_name', 'vendor_rating']].head(10)\n",
    "print(sample_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edcd3c69-80d0-4aa8-a3c7-6e6924e96b3b",
     "showTitle": false,
     "title": ""
    },
    "id": "1c42db87",
    "outputId": "4cc8e2bf-fe10-470e-e306-c92b159037ab"
   },
   "outputs": [],
   "source": [
    "# Analyzing missing values for train data\n",
    "missing_values = train_reduced.isnull().sum()\n",
    "percent_missing = (train_reduced.isnull().sum() / len(train_reduced)) * 100\n",
    "\n",
    "# Displaying the analysis\n",
    "missing_analysis = pd.DataFrame({'missing_values': missing_values, 'percent_missing': percent_missing})\n",
    "print(missing_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8ac3dd7-d1eb-4cd6-b0de-78911928bd63",
     "showTitle": false,
     "title": ""
    },
    "id": "630fb912",
    "outputId": "ae31854e-c141-4e87-902b-2647eddb0cbc"
   },
   "outputs": [],
   "source": [
    "# Analyzing missing values for test data\n",
    "missing_values = train_reduced.isnull().sum()\n",
    "percent_missing = (test_reduced.isnull().sum() / len(test_reduced)) * 100\n",
    "\n",
    "# Displaying the analysis\n",
    "missing_analysis = pd.DataFrame({'missing_values': missing_values, 'percent_missing': percent_missing})\n",
    "print(missing_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af33c277-d8aa-4ffa-9240-faafeb1e96ff",
     "showTitle": false,
     "title": ""
    },
    "id": "0ff7bec7"
   },
   "outputs": [],
   "source": [
    "# Create independent copies to avoid SettingWithCopyWarning\n",
    "train_reduced = train_reduced.copy()\n",
    "test_reduced = test_reduced.copy()\n",
    "\n",
    "# Handling missing values in 'location_type'\n",
    "train_reduced['location_type'].fillna('Unknown', inplace=True)\n",
    "test_reduced['location_type'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Handling missing values in 'latitude_x' and 'longitude_x'\n",
    "train_reduced = train_reduced.dropna(subset=['latitude_x', 'longitude_x'])\n",
    "test_reduced = test_reduced.dropna(subset=['latitude_x', 'longitude_x'])\n",
    "\n",
    "# Save the datasets after handling missing values\n",
    "train_reduced.to_csv(\"C:/Users/Joash/Desktop/recommender/train_reduced_cleaned.csv\", index=False)\n",
    "test_reduced.to_csv(\"C:/Users/Joash/Desktop/recommender/test_reduced_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0f1a7e7-e755-4eae-ae4e-7e5d30419ac6",
     "showTitle": false,
     "title": ""
    },
    "id": "9414aab0",
    "outputId": "0177e01d-f8c6-4eef-f9f5-d4f2548aa978"
   },
   "outputs": [],
   "source": [
    "# Load the cleaned datasets\n",
    "train_reduced_cleaned = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/train_reduced_cleaned.csv\")\n",
    "test_reduced_cleaned = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/test_reduced_cleaned.csv\")\n",
    "\n",
    "# Check for missing values in the training dataset\n",
    "missing_values_train = train_reduced_cleaned.isnull().sum()\n",
    "print(\"Missing values in training dataset:\\n\", missing_values_train)\n",
    "\n",
    "# Check for missing values in the testing dataset\n",
    "missing_values_test = test_reduced_cleaned.isnull().sum()\n",
    "print(\"\\nMissing values in testing dataset:\\n\", missing_values_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fec30ef3-60a1-49b5-ad7d-6df8f1a762a7",
     "showTitle": false,
     "title": ""
    },
    "id": "e3c01bdf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Haversine formula to calculate distance between two lat-lon points\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Earth radius in kilometers\n",
    "    dLat = np.radians(lat2 - lat1)\n",
    "    dLon = np.radians(lon2 - lon1)\n",
    "    a = np.sin(dLat/2) * np.sin(dLat/2) + np.cos(np.radians(lat1)) * np.cos(np.radians(lat2)) * np.sin(dLon/2) * np.sin(dLon/2)\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    distance = R * c\n",
    "    return distance\n",
    "\n",
    "# Load the datasets\n",
    "train_reduced_cleaned = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/train_reduced_cleaned.csv\")\n",
    "test_reduced_cleaned = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/test_reduced_cleaned.csv\")\n",
    "\n",
    "# Apply the function to calculate distance\n",
    "train_reduced_cleaned['distance_customer_to_restaurant'] = train_reduced_cleaned.apply(lambda row: haversine(row['latitude_x'], row['longitude_x'], row['latitude_y'], row['longitude_y']), axis=1)\n",
    "test_reduced_cleaned['distance_customer_to_restaurant'] = test_reduced_cleaned.apply(lambda row: haversine(row['latitude_x'], row['longitude_x'], row['latitude_y'], row['longitude_y']), axis=1)\n",
    "\n",
    "# Save the datasets with the new feature\n",
    "train_reduced_cleaned.to_csv(\"C:/Users/Joash/Desktop/recommender/train_feature_engineered.csv\", index=False)\n",
    "test_reduced_cleaned.to_csv(\"C:/Users/Joash/Desktop/recommender/test_feature_engineered.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10ac4ee2-e810-4e88-aa41-6b3d937c35ae",
     "showTitle": false,
     "title": ""
    },
    "id": "3a4adf17"
   },
   "outputs": [],
   "source": [
    "# Function to vectorize open hours calculation\n",
    "def vectorized_open_hours(from_time1, to_time1, from_time2, to_time2):\n",
    "    from_time1 = pd.to_datetime(from_time1, errors='coerce')\n",
    "    to_time1 = pd.to_datetime(to_time1, errors='coerce')\n",
    "    from_time2 = pd.to_datetime(from_time2, errors='coerce')\n",
    "    to_time2 = pd.to_datetime(to_time2, errors='coerce')\n",
    "\n",
    "    duration1 = np.where(pd.notna(from_time1) & pd.notna(to_time1), (to_time1 - from_time1).dt.total_seconds() / 3600, 0)\n",
    "    duration2 = np.where(pd.notna(from_time2) & pd.notna(to_time2), (to_time2 - from_time2).dt.total_seconds() / 3600, 0)\n",
    "\n",
    "    return duration1 + duration2\n",
    "\n",
    "# Load the datasets\n",
    "train_reduced_cleaned = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/train_feature_engineered.csv\")\n",
    "test_reduced_cleaned = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/test_feature_engineered.csv\")\n",
    "\n",
    "# List of days\n",
    "days = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\n",
    "\n",
    "# Apply vectorized function for each day\n",
    "for day in days:\n",
    "    from_time1_col = f'{day}_from_time1'\n",
    "    to_time1_col = f'{day}_to_time1'\n",
    "    from_time2_col = f'{day}_from_time2'\n",
    "    to_time2_col = f'{day}_to_time2'\n",
    "\n",
    "    train_reduced_cleaned[f'{day}_open_hours'] = vectorized_open_hours(\n",
    "        train_reduced_cleaned[from_time1_col],\n",
    "        train_reduced_cleaned[to_time1_col],\n",
    "        train_reduced_cleaned[from_time2_col],\n",
    "        train_reduced_cleaned[to_time2_col])\n",
    "\n",
    "    test_reduced_cleaned[f'{day}_open_hours'] = vectorized_open_hours(\n",
    "        test_reduced_cleaned[from_time1_col],\n",
    "        test_reduced_cleaned[to_time1_col],\n",
    "        test_reduced_cleaned[from_time2_col],\n",
    "        test_reduced_cleaned[to_time2_col])\n",
    "\n",
    "# Save the datasets with new features\n",
    "train_reduced_cleaned.to_csv(\"C:/Users/Joash/Desktop/recommender/train_full_feature_engineered.csv\", index=False)\n",
    "test_reduced_cleaned.to_csv(\"C:/Users/Joash/Desktop/recommender/test_full_feature_engineered.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fe74110-a8f2-4f2f-9a3c-41cef0ec6c20",
     "showTitle": false,
     "title": ""
    },
    "id": "e2fdd891"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets -TODO change the read and writing to use the pyspark and databrick / dbutils file system\n",
    "train_reduced_cleaned = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/train_full_feature_engineered.csv\")\n",
    "test_reduced_cleaned = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/test_full_feature_engineered.csv\")\n",
    "\n",
    "# Feature: Binary indicator if the customer has multiple orders\n",
    "train_reduced_cleaned['multiple_orders_history'] = (train_reduced_cleaned['display_orders'] > 1).astype(int)\n",
    "test_reduced_cleaned['multiple_orders_history'] = (test_reduced_cleaned['display_orders'] > 1).astype(int)\n",
    "\n",
    "# Save the datasets with the new feature\n",
    "train_reduced_cleaned.to_csv(\"C:/Users/Joash/Desktop/recommender/train_customer_feature_engineered.csv\", index=False)\n",
    "test_reduced_cleaned.to_csv(\"C:/Users/Joash/Desktop/recommender/test_customer_feature_engineered.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "060c6444-b7a5-4ef7-81ea-d174c0ffeb36",
     "showTitle": false,
     "title": ""
    },
    "id": "adbd0a7c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "train_reduced_cleaned = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/train_customer_feature_engineered.csv\")\n",
    "test_reduced_cleaned = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/test_customer_feature_engineered.csv\")\n",
    "\n",
    "# Assuming 'location_number' represents different locations for a customer\n",
    "# Feature: Indicating if the customer uses multiple locations (as a proxy for location density)\n",
    "train_reduced_cleaned['uses_multiple_locations'] = (train_reduced_cleaned['location_number'] > 1).astype(int)\n",
    "test_reduced_cleaned['uses_multiple_locations'] = (test_reduced_cleaned['location_number'] > 1).astype(int)\n",
    "\n",
    "# Save the datasets with the new feature\n",
    "train_reduced_cleaned.to_csv(\"C:/Users/Joash/Desktop/recommender/train_location_density_feature_engineered.csv\", index=False)\n",
    "test_reduced_cleaned.to_csv(\"C:/Users/Joash/Desktop/recommender/test_location_density_feature_engineered.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7df5a5a7-434a-449c-a1a4-790c90ac653d",
     "showTitle": false,
     "title": ""
    },
    "id": "4946bade"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "train_reduced_cleaned = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/train_location_density_feature_engineered.csv\")\n",
    "test_reduced_cleaned = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/test_location_density_feature_engineered.csv\")\n",
    "\n",
    "# Function to calculate cuisine diversity score\n",
    "def calculate_cuisine_diversity(cuisine_list):\n",
    "    # Assuming the cuisine list is a string of comma-separated values\n",
    "    cuisines = cuisine_list.split(',') if pd.notna(cuisine_list) else []\n",
    "    return len(set(cuisines))  # Count of unique cuisines\n",
    "\n",
    "# Apply the function to calculate cuisine diversity\n",
    "train_reduced_cleaned['cuisine_diversity_score'] = train_reduced_cleaned['vendor_tag_name'].apply(calculate_cuisine_diversity)\n",
    "test_reduced_cleaned['cuisine_diversity_score'] = test_reduced_cleaned['vendor_tag_name'].apply(calculate_cuisine_diversity)\n",
    "\n",
    "# Save the datasets with new features\n",
    "train_reduced_cleaned.to_csv(\"C:/Users/Joash/Desktop/recommender/train_cuisine_feature_engineered.csv\", index=False)\n",
    "test_reduced_cleaned.to_csv(\"C:/Users/Joash/Desktop/recommender/test_cuisine_feature_engineered.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2b61ab1-f342-491c-b428-cff9f8cd830c",
     "showTitle": false,
     "title": ""
    },
    "id": "6f4aae42",
    "outputId": "a3895f4d-b8ac-4d9d-967a-a2c8b76dc808"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "train_data = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/train_full_feature_engineered.csv\")\n",
    "\n",
    "# Select time-related columns for inspection\n",
    "time_columns = ['sunday_from_time1', 'sunday_to_time1', 'monday_from_time1', 'monday_to_time1',  # Add other time columns here\n",
    "                # ... continue with other days\n",
    "               ]\n",
    "\n",
    "# Display the first few rows of these columns\n",
    "print(train_data[time_columns].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "790c440b-d7ac-4c06-b9a4-17bd0b89059b",
     "showTitle": false,
     "title": ""
    },
    "id": "a1b9d7d2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Function to convert time string to minutes past midnight\n",
    "def time_to_minutes(time_str):\n",
    "    if pd.isna(time_str):\n",
    "        return None\n",
    "    h, m, s = map(int, time_str.split(':'))\n",
    "    return h * 60 + m\n",
    "\n",
    "# Load the dataset with all features\n",
    "train_data = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/train_cuisine_feature_engineered.csv\")\n",
    "test_data = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/test_cuisine_feature_engineered.csv\")\n",
    "\n",
    "# Convert time columns to numerical format\n",
    "time_columns = [\n",
    "    'sunday_from_time1', 'sunday_to_time1', 'sunday_from_time2',\n",
    "    'sunday_to_time2', 'monday_from_time1', 'monday_to_time1', 'monday_from_time2',\n",
    "    'monday_to_time2', 'tuesday_from_time1', 'tuesday_to_time1', 'tuesday_from_time2',\n",
    "    'tuesday_to_time2', 'wednesday_from_time1', 'wednesday_to_time1', 'wednesday_from_time2',\n",
    "    'wednesday_to_time2', 'thursday_from_time1', 'thursday_to_time1', 'thursday_from_time2',\n",
    "    'thursday_to_time2', 'friday_from_time1', 'friday_to_time1', 'friday_from_time2',\n",
    "    'friday_to_time2', 'saturday_from_time1', 'saturday_to_time1', 'saturday_from_time2',\n",
    "    'saturday_to_time2',\n",
    "               ]\n",
    "for col in time_columns:\n",
    "    train_data[col] = train_data[col].apply(time_to_minutes)\n",
    "    test_data[col] = test_data[col].apply(time_to_minutes)\n",
    "\n",
    "# Update the list of numerical columns to include the time columns\n",
    "numerical_cols = ['latitude_x', 'longitude_x', 'latitude_y', 'longitude_y',\n",
    "                  'delivery_charge', 'serving_distance', 'vendor_rating',\n",
    "                  'distance_customer_to_restaurant', 'cuisine_diversity_score'] + time_columns\n",
    "\n",
    "# Apply StandardScaler\n",
    "scaler = StandardScaler()\n",
    "train_data[numerical_cols] = scaler.fit_transform(train_data[numerical_cols])\n",
    "test_data[numerical_cols] = scaler.transform(test_data[numerical_cols])\n",
    "\n",
    "# Save the fully preprocessed datasets\n",
    "train_data.to_csv(\"C:/Users/Joash/Desktop/recommender/train_preprocessed.csv\", index=False)\n",
    "test_data.to_csv(\"C:/Users/Joash/Desktop/recommender/test_preprocessed.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3eb6d2fd-a4b3-495e-8a00-be82a7b39d7f",
     "showTitle": false,
     "title": ""
    },
    "id": "cca4a5b0",
    "outputId": "4830a2d5-77f0-448c-f0bb-91fb595221a3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "train_data = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/train_preprocessed.csv\")\n",
    "\n",
    "# Check unique values in 'is_open'\n",
    "unique_values = train_data['is_open'].unique()\n",
    "print(\"Unique values in 'is_open':\", unique_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad0d025c-d1f0-4cd9-8c76-97be06ca46c1",
     "showTitle": false,
     "title": ""
    },
    "id": "23f8a04b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the preprocessed datasets\n",
    "train_data = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/train_preprocessed.csv\")\n",
    "test_data = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/test_preprocessed.csv\")\n",
    "\n",
    "# List of categorical columns to be encoded\n",
    "categorical_cols = ['gender', 'language', 'location_type',\n",
    "                    'vendor_category_en', 'vendor_tag_name']\n",
    "\n",
    "# Apply one-hot encoding\n",
    "train_data = pd.get_dummies(train_data, columns=categorical_cols, drop_first=True)\n",
    "test_data = pd.get_dummies(test_data, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Ensure the same set of columns in both datasets\n",
    "train_data, test_data = train_data.align(test_data, join='left', axis=1, fill_value=0)\n",
    "\n",
    "# Save the datasets after encoding\n",
    "train_data.to_csv(\"C:/Users/Joash/Desktop/recommender/train_encoded.csv\", index=False)\n",
    "test_data.to_csv(\"C:/Users/Joash/Desktop/recommender/test_encoded.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38d21a38-0bd0-46dd-8c73-53b10b4de83f",
     "showTitle": false,
     "title": ""
    },
    "id": "e845fdd3",
    "outputId": "a7ee5289-6800-443f-a9f2-2485343c6a4b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the encoded dataset\n",
    "train_data = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/train_encoded.csv\")\n",
    "\n",
    "# List of numerical columns to check for skewness\n",
    "numerical_cols = ['delivery_charge', 'serving_distance', 'vendor_rating',\n",
    "                  'sunday_from_time1', 'sunday_to_time1', 'sunday_from_time2',\n",
    "    'sunday_to_time2', 'monday_from_time1', 'monday_to_time1', 'monday_from_time2',\n",
    "    'monday_to_time2', 'tuesday_from_time1', 'tuesday_to_time1', 'tuesday_from_time2',\n",
    "    'tuesday_to_time2', 'wednesday_from_time1', 'wednesday_to_time1', 'wednesday_from_time2',\n",
    "    'wednesday_to_time2', 'thursday_from_time1', 'thursday_to_time1', 'thursday_from_time2',\n",
    "    'thursday_to_time2', 'friday_from_time1', 'friday_to_time1', 'friday_from_time2',\n",
    "    'friday_to_time2', 'saturday_from_time1', 'saturday_to_time1', 'saturday_from_time2',\n",
    "    'saturday_to_time2','latitude_x', 'longitude_x', 'latitude_y', 'longitude_y' ,\n",
    "                  'distance_customer_to_restaurant', 'cuisine_diversity_score',\n",
    "                 ]\n",
    "\n",
    "# Calculate and print skewness for each numerical column\n",
    "for col in numerical_cols:\n",
    "    skewness = train_data[col].skew()\n",
    "    print(f\"Skewness for {col}: {skewness}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf9de965-a987-44a2-accb-0c16ba855f68",
     "showTitle": false,
     "title": ""
    },
    "id": "34873c1a",
    "outputId": "9fb24319-e2d6-447b-9345-fbbbb07fda47"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "train_data = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/train_encoded.csv\")\n",
    "\n",
    "# Check for non-positive values in skewed columns\n",
    "skewed_cols = ['latitude_x', 'latitude_y', 'longitude_y']\n",
    "for col in skewed_cols:\n",
    "    non_positive_count = train_data[train_data[col] <= 0][col].count()\n",
    "    print(f\"Non-positive values in {col}: {non_positive_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb7c85b2-b4c9-4a83-8062-d115f45b81e8",
     "showTitle": false,
     "title": ""
    },
    "id": "22be4596"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the dataset\n",
    "train_data = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/train_encoded.csv\")\n",
    "test_data = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/test_encoded.csv\")\n",
    "\n",
    "# Columns for Min-Max Normalization\n",
    "normalization_cols = ['latitude_x', 'latitude_y', 'longitude_y']\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Normalize specified columns\n",
    "train_data[normalization_cols] = scaler.fit_transform(train_data[normalization_cols])\n",
    "test_data[normalization_cols] = scaler.transform(test_data[normalization_cols])\n",
    "\n",
    "# Save the datasets after normalization\n",
    "train_data.to_csv(\"C:/Users/Joash/Desktop/recommender/train_normalized.csv\", index=False)\n",
    "test_data.to_csv(\"C:/Users/Joash/Desktop/recommender/test_normalized.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eec4b160-43e2-48e3-9090-0147eb33d3a7",
     "showTitle": false,
     "title": ""
    },
    "id": "47e053e7",
    "outputId": "3ae9dcc9-6b91-4697-c1b3-b54eccb3fd6f"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Load the normalized training dataset\n",
    "train_data = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/train_normalized.csv\")\n",
    "\n",
    "# Separate features and target variable\n",
    "# Assuming 'target' is your label column. Adjust the column name if necessary\n",
    "X = train_data.drop('target', axis=1)  # Drop the target column to create feature set\n",
    "y = train_data['target']  # Target variable\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "# Here, 20% of the data is reserved for validation, and 80% for training\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Optionally, you can check the size of each set\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Validation set size: {X_val.shape}\")\n",
    "\n",
    "# At this point, you can proceed to train your model using X_train and y_train\n",
    "# and validate it using X_val and y_val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec56e050-4527-4f27-90f3-fc1c74027bfd",
     "showTitle": false,
     "title": ""
    },
    "id": "bf405dc9",
    "outputId": "2e4ff9e1-42b7-4203-ef80-8343e0ce4688"
   },
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "print(\"scikit-surprise imported successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "770b351a-5365-4d69-a1a0-78ade7ad11b7",
     "showTitle": false,
     "title": ""
    },
    "id": "85b9b7aa",
    "outputId": "eec5d3d9-5d12-426a-ad25-7cb51466c00c"
   },
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0061b90-aeb7-41ac-a9a0-085024123901",
     "showTitle": false,
     "title": ""
    },
    "id": "4c7dffdc",
    "outputId": "494cfc95-6192-4b6e-fa6a-7bef2106d4e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_data = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/train_normalized.csv\")\n",
    "from surprise import Reader, Dataset, SVD\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "# Reshape the data for Surprise\n",
    "reader = Reader(rating_scale=(0, 1))  # Assuming ratings are binary (0 or 1)\n",
    "data = Dataset.load_from_df(train_data[['customer_id', 'id', 'target']], reader)\n",
    "\n",
    "# Define the SVD algorithm\n",
    "svd = SVD()\n",
    "\n",
    "# Perform cross-validation (you can later use train-test split or full train)\n",
    "cross_validate(svd, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60d48a54-a820-4e1b-8ea7-c424d2d45b22",
     "showTitle": false,
     "title": ""
    },
    "id": "fe985db1",
    "outputId": "342966ac-a70a-4a82-ebcc-b934d71f2ae0"
   },
   "outputs": [],
   "source": [
    "!conda list scikit-surprise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e476630-255a-4786-8110-fd98ffde117e",
     "showTitle": false,
     "title": ""
    },
    "id": "31204a20",
    "outputId": "63760560-0172-4038-9c70-958511977ede"
   },
   "outputs": [],
   "source": [
    "!conda list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb2a613a-e4d1-490a-ade3-120d48b3420d",
     "showTitle": false,
     "title": ""
    },
    "id": "8063beae"
   },
   "outputs": [],
   "source": [
    "#Collaborative Filtering Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d35ef65b-ff05-41e3-a05d-acf13d853409",
     "showTitle": false,
     "title": ""
    },
    "id": "8becd670"
   },
   "outputs": [],
   "source": [
    "from surprise import Reader, Dataset, SVD\n",
    "from surprise.model_selection import cross_validate, train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "train_data = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/train_normalized.csv\")\n",
    "\n",
    "# Prepare the dataset for Surprise\n",
    "reader = Reader(rating_scale=(train_data['target'].min(), train_data['target'].max()))\n",
    "data = Dataset.load_from_df(train_data[['customer_id', 'id', 'target']], reader)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Use SVD algorithm\n",
    "model = SVD()\n",
    "\n",
    "# Train the model\n",
    "model.fit(trainset)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.test(testset)\n",
    "\n",
    "# Calculate and print RMSE\n",
    "accuracy = accuracy.rmse(predictions)\n",
    "print(f'RMSE: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "055b45c5-9293-4432-8dc0-84d477985360",
     "showTitle": false,
     "title": ""
    },
    "id": "a54fec2d"
   },
   "outputs": [],
   "source": [
    "#content based filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b811a86-536b-41b0-a89f-d355411b4326",
     "showTitle": false,
     "title": ""
    },
    "id": "3f6958e3",
    "outputId": "67d0c365-f09b-47f7-acff-1447669b6a97"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"C:/Users/Joash/Desktop/recommender/train_normalized.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few customer IDs\n",
    "print(\"Some customer IDs from the dataset:\")\n",
    "print(data['customer_id'].unique()[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8f22275-a086-48c9-ab56-0c9bc523b5a8",
     "showTitle": false,
     "title": ""
    },
    "id": "7c8e8253",
    "outputId": "be4e51da-db8b-4f6c-8617-a745eb5ab242"
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Function to load and preprocess data\n",
    "def load_and_preprocess_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Selecting relevant features\n",
    "    features = [\n",
    "        'distance_customer_to_restaurant', 'cuisine_diversity_score',\n",
    "        'uses_multiple_locations', 'multiple_orders_history',\n",
    "        'sunday_open_hours', 'monday_open_hours', 'tuesday_open_hours',\n",
    "        'wednesday_open_hours', 'thursday_open_hours', 'friday_open_hours',\n",
    "        'saturday_open_hours', 'latitude_x', 'longitude_x',\n",
    "        'latitude_y', 'longitude_y', 'vendor_rating',\n",
    "        'delivery_charge', 'serving_distance'\n",
    "    ]\n",
    "\n",
    "    # Assuming 'target' is the column to predict\n",
    "    X = data[features]\n",
    "    y = data['target']\n",
    "\n",
    "    # Scaling features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    return X_scaled, y\n",
    "\n",
    "# Function for content-based filtering\n",
    "def content_based_filtering(user_id, data, X_scaled, top_n=10):\n",
    "    # Filter data for the specific user\n",
    "    user_data = data[data['customer_id'] == user_id]\n",
    "    user_index = user_data.index\n",
    "\n",
    "    # Calculate similarity\n",
    "    similarity_scores = cosine_similarity(X_scaled[user_index], X_scaled)\n",
    "\n",
    "    # Create a DataFrame for similarity scores\n",
    "    similarity_df = pd.DataFrame(similarity_scores, columns=data.index, index=user_index)\n",
    "\n",
    "    # Remove the user's own interactions\n",
    "    similarity_df = similarity_df.drop(user_index)\n",
    "\n",
    "    # Get top N similar items\n",
    "    top_items = similarity_df.mean().nlargest(top_n).index\n",
    "\n",
    "    return data.loc[top_items]\n",
    "\n",
    "# Load data\n",
    "file_path = \"C:/Users/Joash/Desktop/recommender/train_normalized.csv\"\n",
    "X_scaled, y = load_and_preprocess_data(file_path)\n",
    "\n",
    "# Load original data to get restaurant details\n",
    "original_data = pd.read_csv(file_path)\n",
    "\n",
    "# Example usage\n",
    "user_id = 'M758NNC'  # Example user ID (replace with a real ID from your dataset)\n",
    "recommended_items = content_based_filtering(user_id, original_data, X_scaled, top_n=10)\n",
    "print(recommended_items)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "077e85b1-707d-4738-af9c-6e481c774820",
     "showTitle": false,
     "title": ""
    },
    "id": "be4e1838"
   },
   "outputs": [],
   "source": [
    "      # Should not be empty\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3437654859072909,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "model_development",
   "widgets": {}
  },
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (surprise_env)",
   "language": "python",
   "name": "surprise_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
