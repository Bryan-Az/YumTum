{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd9e66cc-b5bc-4d63-b08e-a5e627b89e86",
     "showTitle": false,
     "title": ""
    },
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Bryan-Az/YumTum/blob/main/local_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c5d6719-d083-4e52-8451-fd00c679ead9",
     "showTitle": false,
     "title": ""
    },
    "id": "82530497",
    "outputId": "db679210-889d-4de7-880c-03b32d5ed0b4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "train_full_path = \"C:/Users/Joash/Desktop/recommender/train_full.csv\"\n",
    "test_full_path = \"C:/Users/Joash/Desktop/recommender/test_full.csv\"\n",
    "\n",
    "train_full = pd.read_csv(train_full_path)\n",
    "test_full = pd.read_csv(test_full_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b68255d9-af91-499e-b31a-4b83fa8d11d7",
     "showTitle": false,
     "title": ""
    },
    "id": "a4bc223d",
    "outputId": "faf62172-2ac3-4045-acf4-2165cdc0f156"
   },
   "outputs": [],
   "source": [
    "train_full.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20ca8d35-425b-43d3-9fb4-15064faab443",
     "showTitle": false,
     "title": ""
    },
    "id": "0b7de124",
    "outputId": "bd77b279-a7b8-4c73-dbc8-023d8a9a9712"
   },
   "outputs": [],
   "source": [
    "test_full.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9fafd553-6827-4146-9cf9-158f04e5f94b",
     "showTitle": false,
     "title": ""
    },
    "id": "83152f7a"
   },
   "outputs": [],
   "source": [
    "#Sampling\n",
    "\n",
    "# Define a comprehensive set of stratifying variables\n",
    "stratifying_vars = ['gender', 'language', 'customer_location_type', 'restaurant_location_type', 'favorite_cuisine_type']\n",
    "\n",
    "# Function to perform stratified sampling\n",
    "def stratified_sample(df, stratify_cols, frac=0.25):\n",
    "    # Ensure all stratifying columns are present in the dataframe\n",
    "    stratify_cols = [col for col in stratify_cols if col in df.columns]\n",
    "    return df.groupby(stratify_cols, group_keys=False).apply(lambda x: x.sample(frac=frac))\n",
    "\n",
    "# Applying stratified sampling on train and test datasets\n",
    "train_sampled = stratified_sample(train_full, stratifying_vars, frac=0.25)\n",
    "test_sampled = stratified_sample(test_full, stratifying_vars, frac=0.25)\n",
    "\n",
    "# Saving the sampled data\n",
    "train_sampled.to_csv(\"C:/Users/Joash/Desktop/recommender/train_sampled.csv\", index=False)\n",
    "test_sampled.to_csv(\"C:/Users/Joash/Desktop/recommender/test_sampled.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f04bdc3f-4678-46c5-a894-1d0da11602a5",
     "showTitle": false,
     "title": ""
    },
    "id": "9aea88b7",
    "outputId": "37861790-98f4-4fd4-fb65-fa47d48fbaec"
   },
   "outputs": [],
   "source": [
    "# Print column names of the train dataset\n",
    "print(train_sampled.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "663b27eb-c788-4e0d-8b9b-ed8f804cf9a5",
     "showTitle": false,
     "title": ""
    },
    "id": "646a7897",
    "outputId": "f755ec09-3225-4d10-f190-405ef84f0bee"
   },
   "outputs": [],
   "source": [
    "# Print column names of the test dataset\n",
    "print(test_sampled.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8a5e066-8140-405f-87f1-df167e9c9f37",
     "showTitle": false,
     "title": ""
    },
    "id": "6e05a3ad"
   },
   "outputs": [],
   "source": [
    "#Feature reduction\n",
    "# Updated columns to keep for the training dataset\n",
    "train_columns_to_keep = [\n",
    "    'customer_id', 'gender', 'language',\n",
    "    'location_number', 'location_type', 'latitude_x', 'longitude_x',\n",
    "    'id', 'latitude_y', 'longitude_y', 'vendor_category_en', 'vendor_rating',\n",
    "    'delivery_charge', 'serving_distance', 'is_open', 'discount_percentage',\n",
    "    'display_orders',\n",
    "    'vendor_tag_name','sunday_from_time1', 'sunday_to_time1', 'sunday_from_time2',\n",
    "    'sunday_to_time2', 'monday_from_time1', 'monday_to_time1', 'monday_from_time2',\n",
    "    'monday_to_time2', 'tuesday_from_time1', 'tuesday_to_time1', 'tuesday_from_time2',\n",
    "    'tuesday_to_time2', 'wednesday_from_time1', 'wednesday_to_time1', 'wednesday_from_time2',\n",
    "    'wednesday_to_time2', 'thursday_from_time1', 'thursday_to_time1', 'thursday_from_time2',\n",
    "    'thursday_to_time2', 'friday_from_time1', 'friday_to_time1', 'friday_from_time2',\n",
    "    'friday_to_time2', 'saturday_from_time1', 'saturday_to_time1', 'saturday_from_time2',\n",
    "    'saturday_to_time2',\n",
    "    'target'  # 'target' column is specific to the training dataset\n",
    "]\n",
    "\n",
    "# Updated columns to keep for the testing dataset\n",
    "test_columns_to_keep = [\n",
    "    'customer_id', 'gender', 'language',\n",
    "    'location_number', 'location_type', 'latitude_x', 'longitude_x',\n",
    "    'id', 'latitude_y', 'longitude_y', 'vendor_category_en', 'vendor_rating',\n",
    "    'delivery_charge', 'serving_distance', 'is_open', 'discount_percentage',\n",
    "    'display_orders',  # Including 'display_orders'\n",
    "    'vendor_tag_name','sunday_from_time1', 'sunday_to_time1', 'sunday_from_time2',\n",
    "    'sunday_to_time2', 'monday_from_time1', 'monday_to_time1', 'monday_from_time2',\n",
    "    'monday_to_time2', 'tuesday_from_time1', 'tuesday_to_time1', 'tuesday_from_time2',\n",
    "    'tuesday_to_time2', 'wednesday_from_time1', 'wednesday_to_time1', 'wednesday_from_time2',\n",
    "    'wednesday_to_time2', 'thursday_from_time1', 'thursday_to_time1', 'thursday_from_time2',\n",
    "    'thursday_to_time2', 'friday_from_time1', 'friday_to_time1', 'friday_from_time2', 'friday_to_time2',\n",
    "    'saturday_from_time1', 'saturday_to_time1', 'saturday_from_time2', 'saturday_to_time2'\n",
    "]\n",
    "\n",
    "\n",
    "# Reduce features in the training dataset\n",
    "train_reduced = train_sampled[train_columns_to_keep]\n",
    "\n",
    "# Reduce features in the testing dataset\n",
    "test_reduced = test_sampled[test_columns_to_keep]\n",
    "\n",
    "# Save the reduced datasets\n",
    "train_reduced.to_csv(\"C:/Users/Joash/Desktop/recommender/train_reduced.csv\", index=False)\n",
    "test_reduced.to_csv(\"C:/Users/Joash/Desktop/recommender/test_reduced.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9525f2ca-1ebd-48d3-9549-c95b20540812",
     "showTitle": false,
     "title": ""
    },
    "id": "4a2d3c9d",
    "outputId": "c8822f1d-18bf-4a1a-aac6-eae81ceb0242"
   },
   "outputs": [],
   "source": [
    "train_reduced.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8bbdc9a-3d8b-40e1-97fa-fb4327d1bc85",
     "showTitle": false,
     "title": ""
    },
    "id": "c73c234f",
    "outputId": "ba64e96e-a5fc-459f-bda9-62943b3215e4"
   },
   "outputs": [],
   "source": [
    "test_reduced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3dc8db9-5f16-44b6-81b4-dada74a82b7c",
     "showTitle": false,
     "title": ""
    },
    "id": "360ab9d2",
    "outputId": "4da56bc4-c591-4461-926e-e26a8354a47a"
   },
   "outputs": [],
   "source": [
    "len(train_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82d7bf27-cdae-4ab9-815c-7aeae1b9d5ed",
     "showTitle": false,
     "title": ""
    },
    "id": "25bc5697",
    "outputId": "8a92a6f3-4e6a-4d23-959d-1a3f1690fe50"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "train_data = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/train_reduced_cleaned.csv\")\n",
    "\n",
    "# Display a sample of the specific columns\n",
    "sample_data = train_data[['vendor_category_en', 'vendor_tag_name', 'vendor_rating']].head(10)\n",
    "print(sample_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d2414c9-d23f-4abe-9c44-74c8258506c3",
     "showTitle": false,
     "title": ""
    },
    "id": "1c42db87",
    "outputId": "ae5faf27-0df3-4363-ff89-6ad69840f175"
   },
   "outputs": [],
   "source": [
    "# Analyzing missing values for train data\n",
    "missing_values = train_reduced.isnull().sum()\n",
    "percent_missing = (train_reduced.isnull().sum() / len(train_reduced)) * 100\n",
    "\n",
    "# Displaying the analysis\n",
    "missing_analysis = pd.DataFrame({'missing_values': missing_values, 'percent_missing': percent_missing})\n",
    "print(missing_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f70f3cd-6220-41b4-abc3-dd926ca74841",
     "showTitle": false,
     "title": ""
    },
    "id": "630fb912",
    "outputId": "d793fa52-84e1-427e-d9d6-aa68c7b3e0ac"
   },
   "outputs": [],
   "source": [
    "# Analyzing missing values for test data\n",
    "missing_values = train_reduced.isnull().sum()\n",
    "percent_missing = (test_reduced.isnull().sum() / len(test_reduced)) * 100\n",
    "\n",
    "# Displaying the analysis\n",
    "missing_analysis = pd.DataFrame({'missing_values': missing_values, 'percent_missing': percent_missing})\n",
    "print(missing_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8503bc05-1b1d-4c83-9fa0-4542573376aa",
     "showTitle": false,
     "title": ""
    },
    "id": "0ff7bec7"
   },
   "outputs": [],
   "source": [
    "# Create independent copies to avoid SettingWithCopyWarning\n",
    "train_reduced = train_reduced.copy()\n",
    "test_reduced = test_reduced.copy()\n",
    "\n",
    "# Handling missing values in 'location_type'\n",
    "train_reduced['location_type'].fillna('Unknown', inplace=True)\n",
    "test_reduced['location_type'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Handling missing values in 'latitude_x' and 'longitude_x'\n",
    "train_reduced = train_reduced.dropna(subset=['latitude_x', 'longitude_x'])\n",
    "test_reduced = test_reduced.dropna(subset=['latitude_x', 'longitude_x'])\n",
    "\n",
    "# Save the datasets after handling missing values\n",
    "train_reduced.to_csv(\"C:/Users/Joash/Desktop/recommender/train_reduced_cleaned.csv\", index=False)\n",
    "test_reduced.to_csv(\"C:/Users/Joash/Desktop/recommender/test_reduced_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f67e0c1-b6a8-426c-abf1-230ed2cbf91a",
     "showTitle": false,
     "title": ""
    },
    "id": "9414aab0",
    "outputId": "11aeab40-482b-4c27-b3ef-2603fcc6e46b"
   },
   "outputs": [],
   "source": [
    "# Load the cleaned datasets\n",
    "train_reduced_cleaned = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/train_reduced_cleaned.csv\")\n",
    "test_reduced_cleaned = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/test_reduced_cleaned.csv\")\n",
    "\n",
    "# Check for missing values in the training dataset\n",
    "missing_values_train = train_reduced_cleaned.isnull().sum()\n",
    "print(\"Missing values in training dataset:\\n\", missing_values_train)\n",
    "\n",
    "# Check for missing values in the testing dataset\n",
    "missing_values_test = test_reduced_cleaned.isnull().sum()\n",
    "print(\"\\nMissing values in testing dataset:\\n\", missing_values_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14d302d5-fed5-4920-b9e3-57a6ba0e61fc",
     "showTitle": false,
     "title": ""
    },
    "id": "e3c01bdf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Haversine formula to calculate distance between two lat-lon points\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Earth radius in kilometers\n",
    "    dLat = np.radians(lat2 - lat1)\n",
    "    dLon = np.radians(lon2 - lon1)\n",
    "    a = np.sin(dLat/2) * np.sin(dLat/2) + np.cos(np.radians(lat1)) * np.cos(np.radians(lat2)) * np.sin(dLon/2) * np.sin(dLon/2)\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    distance = R * c\n",
    "    return distance\n",
    "\n",
    "# Load the datasets\n",
    "train_reduced_cleaned = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/train_reduced_cleaned.csv\")\n",
    "test_reduced_cleaned = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/test_reduced_cleaned.csv\")\n",
    "\n",
    "# Apply the function to calculate distance\n",
    "train_reduced_cleaned['distance_customer_to_restaurant'] = train_reduced_cleaned.apply(lambda row: haversine(row['latitude_x'], row['longitude_x'], row['latitude_y'], row['longitude_y']), axis=1)\n",
    "test_reduced_cleaned['distance_customer_to_restaurant'] = test_reduced_cleaned.apply(lambda row: haversine(row['latitude_x'], row['longitude_x'], row['latitude_y'], row['longitude_y']), axis=1)\n",
    "\n",
    "# Save the datasets with the new feature\n",
    "train_reduced_cleaned.to_csv(\"C:/Users/Joash/Desktop/recommender/train_feature_engineered.csv\", index=False)\n",
    "test_reduced_cleaned.to_csv(\"C:/Users/Joash/Desktop/recommender/test_feature_engineered.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00c1826b-c6d5-43ee-8b26-34bc80d481c8",
     "showTitle": false,
     "title": ""
    },
    "id": "3a4adf17"
   },
   "outputs": [],
   "source": [
    "# Function to vectorize open hours calculation\n",
    "def vectorized_open_hours(from_time1, to_time1, from_time2, to_time2):\n",
    "    from_time1 = pd.to_datetime(from_time1, errors='coerce')\n",
    "    to_time1 = pd.to_datetime(to_time1, errors='coerce')\n",
    "    from_time2 = pd.to_datetime(from_time2, errors='coerce')\n",
    "    to_time2 = pd.to_datetime(to_time2, errors='coerce')\n",
    "\n",
    "    duration1 = np.where(pd.notna(from_time1) & pd.notna(to_time1), (to_time1 - from_time1).dt.total_seconds() / 3600, 0)\n",
    "    duration2 = np.where(pd.notna(from_time2) & pd.notna(to_time2), (to_time2 - from_time2).dt.total_seconds() / 3600, 0)\n",
    "\n",
    "    return duration1 + duration2\n",
    "\n",
    "# Load the datasets\n",
    "train_reduced_cleaned = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/train_feature_engineered.csv\")\n",
    "test_reduced_cleaned = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/test_feature_engineered.csv\")\n",
    "\n",
    "# List of days\n",
    "days = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\n",
    "\n",
    "# Apply vectorized function for each day\n",
    "for day in days:\n",
    "    from_time1_col = f'{day}_from_time1'\n",
    "    to_time1_col = f'{day}_to_time1'\n",
    "    from_time2_col = f'{day}_from_time2'\n",
    "    to_time2_col = f'{day}_to_time2'\n",
    "\n",
    "    train_reduced_cleaned[f'{day}_open_hours'] = vectorized_open_hours(\n",
    "        train_reduced_cleaned[from_time1_col],\n",
    "        train_reduced_cleaned[to_time1_col],\n",
    "        train_reduced_cleaned[from_time2_col],\n",
    "        train_reduced_cleaned[to_time2_col])\n",
    "\n",
    "    test_reduced_cleaned[f'{day}_open_hours'] = vectorized_open_hours(\n",
    "        test_reduced_cleaned[from_time1_col],\n",
    "        test_reduced_cleaned[to_time1_col],\n",
    "        test_reduced_cleaned[from_time2_col],\n",
    "        test_reduced_cleaned[to_time2_col])\n",
    "\n",
    "# Save the datasets with new features\n",
    "train_reduced_cleaned.to_csv(\"C:/Users/Joash/Desktop/recommender/train_full_feature_engineered.csv\", index=False)\n",
    "test_reduced_cleaned.to_csv(\"C:/Users/Joash/Desktop/recommender/test_full_feature_engineered.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44fad4d2-6bba-4cf0-b58d-e0267eee1a2a",
     "showTitle": false,
     "title": ""
    },
    "id": "e2fdd891"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "train_reduced_cleaned = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/train_full_feature_engineered.csv\")\n",
    "test_reduced_cleaned = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/test_full_feature_engineered.csv\")\n",
    "\n",
    "# Feature: Binary indicator if the customer has multiple orders\n",
    "train_reduced_cleaned['multiple_orders_history'] = (train_reduced_cleaned['display_orders'] > 1).astype(int)\n",
    "test_reduced_cleaned['multiple_orders_history'] = (test_reduced_cleaned['display_orders'] > 1).astype(int)\n",
    "\n",
    "# Save the datasets with the new feature\n",
    "train_reduced_cleaned.to_csv(\"C:/Users/Joash/Desktop/recommender/train_customer_feature_engineered.csv\", index=False)\n",
    "test_reduced_cleaned.to_csv(\"C:/Users/Joash/Desktop/recommender/test_customer_feature_engineered.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f5997c6-3a54-4fb0-9438-e8a2d0971163",
     "showTitle": false,
     "title": ""
    },
    "id": "adbd0a7c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "train_reduced_cleaned = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/train_customer_feature_engineered.csv\")\n",
    "test_reduced_cleaned = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/test_customer_feature_engineered.csv\")\n",
    "\n",
    "# Assuming 'location_number' represents different locations for a customer\n",
    "# Feature: Indicating if the customer uses multiple locations (as a proxy for location density)\n",
    "train_reduced_cleaned['uses_multiple_locations'] = (train_reduced_cleaned['location_number'] > 1).astype(int)\n",
    "test_reduced_cleaned['uses_multiple_locations'] = (test_reduced_cleaned['location_number'] > 1).astype(int)\n",
    "\n",
    "# Save the datasets with the new feature\n",
    "train_reduced_cleaned.to_csv(\"C:/Users/Joash/Desktop/recommender/train_location_density_feature_engineered.csv\", index=False)\n",
    "test_reduced_cleaned.to_csv(\"C:/Users/Joash/Desktop/recommender/test_location_density_feature_engineered.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97e2e125-7675-40c0-a705-2ffc12daca32",
     "showTitle": false,
     "title": ""
    },
    "id": "4946bade"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "train_reduced_cleaned = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/train_location_density_feature_engineered.csv\")\n",
    "test_reduced_cleaned = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/test_location_density_feature_engineered.csv\")\n",
    "\n",
    "# Function to calculate cuisine diversity score\n",
    "def calculate_cuisine_diversity(cuisine_list):\n",
    "    # Assuming the cuisine list is a string of comma-separated values\n",
    "    cuisines = cuisine_list.split(',') if pd.notna(cuisine_list) else []\n",
    "    return len(set(cuisines))  # Count of unique cuisines\n",
    "\n",
    "# Apply the function to calculate cuisine diversity\n",
    "train_reduced_cleaned['cuisine_diversity_score'] = train_reduced_cleaned['vendor_tag_name'].apply(calculate_cuisine_diversity)\n",
    "test_reduced_cleaned['cuisine_diversity_score'] = test_reduced_cleaned['vendor_tag_name'].apply(calculate_cuisine_diversity)\n",
    "\n",
    "# Save the datasets with new features\n",
    "train_reduced_cleaned.to_csv(\"C:/Users/Joash/Desktop/recommender/train_cuisine_feature_engineered.csv\", index=False)\n",
    "test_reduced_cleaned.to_csv(\"C:/Users/Joash/Desktop/recommender/test_cuisine_feature_engineered.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "352f3f2d-75e6-411e-95a2-093a3af9cc0d",
     "showTitle": false,
     "title": ""
    },
    "id": "6f4aae42",
    "outputId": "838f088a-53db-49ba-fc5b-0f800178e372"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "train_data = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/train_full_feature_engineered.csv\")\n",
    "\n",
    "# Select time-related columns for inspection\n",
    "time_columns = ['sunday_from_time1', 'sunday_to_time1', 'monday_from_time1', 'monday_to_time1',  # Add other time columns here\n",
    "                # ... continue with other days\n",
    "               ]\n",
    "\n",
    "# Display the first few rows of these columns\n",
    "print(train_data[time_columns].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01710e33-8452-4777-8dd7-1de7928f1082",
     "showTitle": false,
     "title": ""
    },
    "id": "a1b9d7d2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Function to convert time string to minutes past midnight\n",
    "def time_to_minutes(time_str):\n",
    "    if pd.isna(time_str):\n",
    "        return None\n",
    "    h, m, s = map(int, time_str.split(':'))\n",
    "    return h * 60 + m\n",
    "\n",
    "# Load the dataset with all features\n",
    "train_data = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/train_cuisine_feature_engineered.csv\")\n",
    "test_data = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/test_cuisine_feature_engineered.csv\")\n",
    "\n",
    "# Convert time columns to numerical format\n",
    "time_columns = [\n",
    "    'sunday_from_time1', 'sunday_to_time1', 'sunday_from_time2',\n",
    "    'sunday_to_time2', 'monday_from_time1', 'monday_to_time1', 'monday_from_time2',\n",
    "    'monday_to_time2', 'tuesday_from_time1', 'tuesday_to_time1', 'tuesday_from_time2',\n",
    "    'tuesday_to_time2', 'wednesday_from_time1', 'wednesday_to_time1', 'wednesday_from_time2',\n",
    "    'wednesday_to_time2', 'thursday_from_time1', 'thursday_to_time1', 'thursday_from_time2',\n",
    "    'thursday_to_time2', 'friday_from_time1', 'friday_to_time1', 'friday_from_time2',\n",
    "    'friday_to_time2', 'saturday_from_time1', 'saturday_to_time1', 'saturday_from_time2',\n",
    "    'saturday_to_time2',\n",
    "               ]\n",
    "for col in time_columns:\n",
    "    train_data[col] = train_data[col].apply(time_to_minutes)\n",
    "    test_data[col] = test_data[col].apply(time_to_minutes)\n",
    "\n",
    "# Update the list of numerical columns to include the time columns\n",
    "numerical_cols = ['latitude_x', 'longitude_x', 'latitude_y', 'longitude_y',\n",
    "                  'delivery_charge', 'serving_distance', 'vendor_rating',\n",
    "                  'distance_customer_to_restaurant', 'cuisine_diversity_score'] + time_columns\n",
    "\n",
    "# Apply StandardScaler\n",
    "scaler = StandardScaler()\n",
    "train_data[numerical_cols] = scaler.fit_transform(train_data[numerical_cols])\n",
    "test_data[numerical_cols] = scaler.transform(test_data[numerical_cols])\n",
    "\n",
    "# Save the fully preprocessed datasets\n",
    "train_data.to_csv(\"C:/Users/Joash/Desktop/recommender/train_preprocessed.csv\", index=False)\n",
    "test_data.to_csv(\"C:/Users/Joash/Desktop/recommender/test_preprocessed.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "462a1d3f-4a09-4f7a-af80-1c29fe012414",
     "showTitle": false,
     "title": ""
    },
    "id": "cca4a5b0",
    "outputId": "3e67363f-2661-4cc8-b90c-2c89a0a4965b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "train_data = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/train_preprocessed.csv\")\n",
    "\n",
    "# Check unique values in 'is_open'\n",
    "unique_values = train_data['is_open'].unique()\n",
    "print(\"Unique values in 'is_open':\", unique_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "731e64df-ec62-4f66-af5b-dcf05597be19",
     "showTitle": false,
     "title": ""
    },
    "id": "23f8a04b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the preprocessed datasets\n",
    "train_data = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/train_preprocessed.csv\")\n",
    "test_data = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/test_preprocessed.csv\")\n",
    "\n",
    "# List of categorical columns to be encoded\n",
    "categorical_cols = ['gender', 'language', 'location_type',\n",
    "                    'vendor_category_en', 'vendor_tag_name']\n",
    "\n",
    "# Apply one-hot encoding\n",
    "train_data = pd.get_dummies(train_data, columns=categorical_cols, drop_first=True)\n",
    "test_data = pd.get_dummies(test_data, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Ensure the same set of columns in both datasets\n",
    "train_data, test_data = train_data.align(test_data, join='left', axis=1, fill_value=0)\n",
    "\n",
    "# Save the datasets after encoding\n",
    "train_data.to_csv(\"C:/Users/Joash/Desktop/recommender/train_encoded.csv\", index=False)\n",
    "test_data.to_csv(\"C:/Users/Joash/Desktop/recommender/test_encoded.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35fcd8d6-7a8f-47fd-a72c-5b74bbb44e0f",
     "showTitle": false,
     "title": ""
    },
    "id": "e845fdd3",
    "outputId": "e864fa8f-52e0-4603-add6-5690b53847d0"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the encoded dataset\n",
    "train_data = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/train_encoded.csv\")\n",
    "\n",
    "# List of numerical columns to check for skewness\n",
    "numerical_cols = ['delivery_charge', 'serving_distance', 'vendor_rating',\n",
    "                  'sunday_from_time1', 'sunday_to_time1', 'sunday_from_time2',\n",
    "    'sunday_to_time2', 'monday_from_time1', 'monday_to_time1', 'monday_from_time2',\n",
    "    'monday_to_time2', 'tuesday_from_time1', 'tuesday_to_time1', 'tuesday_from_time2',\n",
    "    'tuesday_to_time2', 'wednesday_from_time1', 'wednesday_to_time1', 'wednesday_from_time2',\n",
    "    'wednesday_to_time2', 'thursday_from_time1', 'thursday_to_time1', 'thursday_from_time2',\n",
    "    'thursday_to_time2', 'friday_from_time1', 'friday_to_time1', 'friday_from_time2',\n",
    "    'friday_to_time2', 'saturday_from_time1', 'saturday_to_time1', 'saturday_from_time2',\n",
    "    'saturday_to_time2','latitude_x', 'longitude_x', 'latitude_y', 'longitude_y' ,\n",
    "                  'distance_customer_to_restaurant', 'cuisine_diversity_score',\n",
    "                 ]\n",
    "\n",
    "# Calculate and print skewness for each numerical column\n",
    "for col in numerical_cols:\n",
    "    skewness = train_data[col].skew()\n",
    "    print(f\"Skewness for {col}: {skewness}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3b816fd-7a2c-4ca2-92cd-b68073e0ce23",
     "showTitle": false,
     "title": ""
    },
    "id": "34873c1a",
    "outputId": "15b2a0b7-a11a-4f10-88d1-3e1743f59d5a"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "train_data = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/train_encoded.csv\")\n",
    "\n",
    "# Check for non-positive values in skewed columns\n",
    "skewed_cols = ['latitude_x', 'latitude_y', 'longitude_y']\n",
    "for col in skewed_cols:\n",
    "    non_positive_count = train_data[train_data[col] <= 0][col].count()\n",
    "    print(f\"Non-positive values in {col}: {non_positive_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36ac57e1-033a-47c2-90a1-18f0a97ed02f",
     "showTitle": false,
     "title": ""
    },
    "id": "22be4596"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the dataset\n",
    "train_data = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/train_encoded.csv\")\n",
    "test_data = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/test_encoded.csv\")\n",
    "\n",
    "# Columns for Min-Max Normalization\n",
    "normalization_cols = ['latitude_x', 'latitude_y', 'longitude_y']\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Normalize specified columns\n",
    "train_data[normalization_cols] = scaler.fit_transform(train_data[normalization_cols])\n",
    "test_data[normalization_cols] = scaler.transform(test_data[normalization_cols])\n",
    "\n",
    "# Save the datasets after normalization\n",
    "train_data.to_csv(\"C:/Users/Joash/Desktop/recommender/train_normalized.csv\", index=False)\n",
    "test_data.to_csv(\"C:/Users/Joash/Desktop/recommender/test_normalized.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26077dbc-b5f6-4376-b354-a6bd6bba26d2",
     "showTitle": false,
     "title": ""
    },
    "id": "47e053e7",
    "outputId": "e73050ef-d5f9-4f2d-8cce-25476f11d3a7"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Load the normalized training dataset\n",
    "train_data = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/train_normalized.csv\")\n",
    "\n",
    "# Separate features and target variable\n",
    "# Assuming 'target' is your label column. Adjust the column name if necessary\n",
    "X = train_data.drop('target', axis=1)  # Drop the target column to create feature set\n",
    "y = train_data['target']  # Target variable\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "# Here, 20% of the data is reserved for validation, and 80% for training\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Optionally, you can check the size of each set\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Validation set size: {X_val.shape}\")\n",
    "\n",
    "# At this point, you can proceed to train your model using X_train and y_train\n",
    "# and validate it using X_val and y_val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca208b21-521e-45f9-8d51-15debcbee3a6",
     "showTitle": false,
     "title": ""
    },
    "id": "bf405dc9",
    "outputId": "346344f5-540b-4b47-d344-7e35198057be"
   },
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "print(\"scikit-surprise imported successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "916b8f51-d380-4334-b534-5804d4699eae",
     "showTitle": false,
     "title": ""
    },
    "id": "22078287",
    "outputId": "870577fb-173d-48c0-e769-cb11639d22c1"
   },
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79a84b97-8325-46d7-b4af-06b77f1e4a44",
     "showTitle": false,
     "title": ""
    },
    "id": "5b2455df"
   },
   "outputs": [],
   "source": [
    "#Collaborative Filtering Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cbf010b-0509-4294-8dbb-db78a74a18bd",
     "showTitle": false,
     "title": ""
    },
    "id": "4c7dffdc",
    "outputId": "74de59be-5b66-4ae4-bc0c-9300a8ef4f60"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_data = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/train_normalized.csv\")\n",
    "from surprise import Reader, Dataset, SVD\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "# Reshape the data for Surprise\n",
    "reader = Reader(rating_scale=(0, 1))  # Assuming ratings are binary (0 or 1)\n",
    "data = Dataset.load_from_df(train_data[['customer_id', 'id', 'target']], reader)\n",
    "\n",
    "# Define the SVD algorithm\n",
    "svd = SVD()\n",
    "\n",
    "# Perform cross-validation (you can later use train-test split or full train)\n",
    "cross_validate(svd, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "315d062f-a6df-4ec6-8c1f-98f8a2ea5849",
     "showTitle": false,
     "title": ""
    },
    "id": "e75ab662",
    "outputId": "8e06a0d0-f9c5-4d18-efb3-ae379dbde093"
   },
   "outputs": [],
   "source": [
    "!conda list scikit-surprise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc5d9bac-c0b4-4f56-a9bb-437e3ad60471",
     "showTitle": false,
     "title": ""
    },
    "id": "31204a20",
    "outputId": "33aa14f2-710a-46b9-fc2b-dfc8b55a5cc2"
   },
   "outputs": [],
   "source": [
    "!conda list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "187a46a1-5966-4f41-a11b-d53f5b3f9a36",
     "showTitle": false,
     "title": ""
    },
    "id": "087926ee"
   },
   "outputs": [],
   "source": [
    "#content based filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e28ceb5-1d71-4ac5-b260-edfa5cbf1c08",
     "showTitle": false,
     "title": ""
    },
    "id": "bea367e5",
    "outputId": "b33378fd-b4bd-4f94-ed56-9b3f751eaeb3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"C:/Users/Joash/Desktop/recommender/train_normalized.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few customer IDs\n",
    "print(\"Some customer IDs from the dataset:\")\n",
    "print(data['customer_id'].unique()[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d80a40cb-a6a9-486b-82d1-408507adbabb",
     "showTitle": false,
     "title": ""
    },
    "id": "90bdc501",
    "outputId": "7376a69f-7b8e-4508-a3e2-741064782fb3"
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Function to load and preprocess data\n",
    "def load_and_preprocess_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Selecting relevant features\n",
    "    features = [\n",
    "        'distance_customer_to_restaurant', 'cuisine_diversity_score',\n",
    "        'uses_multiple_locations', 'multiple_orders_history',\n",
    "        'sunday_open_hours', 'monday_open_hours', 'tuesday_open_hours',\n",
    "        'wednesday_open_hours', 'thursday_open_hours', 'friday_open_hours',\n",
    "        'saturday_open_hours', 'latitude_x', 'longitude_x',\n",
    "        'latitude_y', 'longitude_y', 'vendor_rating',\n",
    "        'delivery_charge', 'serving_distance'\n",
    "    ]\n",
    "\n",
    "    # Assuming 'target' is the column to predict\n",
    "    X = data[features]\n",
    "    y = data['target']\n",
    "\n",
    "    # Scaling features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    return X_scaled, y\n",
    "\n",
    "# Function for content-based filtering\n",
    "def content_based_filtering(user_id, data, X_scaled, top_n=10):\n",
    "    # Filter data for the specific user\n",
    "    user_data = data[data['customer_id'] == user_id]\n",
    "    user_index = user_data.index\n",
    "\n",
    "    # Calculate similarity\n",
    "    similarity_scores = cosine_similarity(X_scaled[user_index], X_scaled)\n",
    "\n",
    "    # Create a DataFrame for similarity scores\n",
    "    similarity_df = pd.DataFrame(similarity_scores, columns=data.index, index=user_index)\n",
    "\n",
    "    # Remove the user's own interactions\n",
    "    similarity_df = similarity_df.drop(user_index)\n",
    "\n",
    "    # Get top N similar items\n",
    "    top_items = similarity_df.mean().nlargest(top_n).index\n",
    "\n",
    "    return data.loc[top_items]\n",
    "\n",
    "# Load data\n",
    "file_path = \"C:/Users/Joash/Desktop/recommender/train_normalized.csv\"\n",
    "X_scaled, y = load_and_preprocess_data(file_path)\n",
    "\n",
    "# Load original data to get restaurant details\n",
    "original_data = pd.read_csv(file_path)\n",
    "\n",
    "# Example usage\n",
    "user_id = 'M758NNC'  # Example user ID (replace with a real ID from your dataset)\n",
    "recommended_items = content_based_filtering(user_id, original_data, X_scaled, top_n=10)\n",
    "print(recommended_items)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5bf8604d-1767-4f20-9bb5-4d030f9ba868",
     "showTitle": false,
     "title": ""
    },
    "id": "c5ab6195"
   },
   "outputs": [],
   "source": [
    "#Collaborative Filtering Implementation using train-test split and calculating RMSE on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a008dbd-e5b6-43aa-b182-082aa4a83446",
     "showTitle": false,
     "title": ""
    },
    "id": "94c5b00f",
    "outputId": "c6483572-8716-420a-f9f6-3f9e5eeb49f8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from surprise import Reader, Dataset, SVD\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy  # Import the accuracy module\n",
    "\n",
    "# Load the dataset\n",
    "train_data = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/train_normalized.csv\")\n",
    "\n",
    "# Prepare the dataset for Surprise\n",
    "reader = Reader(rating_scale=(train_data['target'].min(), train_data['target'].max()))\n",
    "data = Dataset.load_from_df(train_data[['customer_id', 'id', 'target']], reader)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Use the SVD algorithm\n",
    "model = SVD()\n",
    "\n",
    "# Train the model\n",
    "model.fit(trainset)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.test(testset)\n",
    "\n",
    "# Calculate and print RMSE\n",
    "accuracy_value = accuracy.rmse(predictions)\n",
    "print(f'RMSE: {accuracy_value}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d42dcd8b-ad8e-4ffa-bc00-9ac33da0a84c",
     "showTitle": false,
     "title": ""
    },
    "id": "a016f6a2",
    "outputId": "6a4bc46d-a2be-4506-a8df-74f539111300"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"C:/Users/Joash/Desktop/recommender/test_normalized.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few customer IDs\n",
    "print(\"Some customer IDs from the dataset:\")\n",
    "print(data['customer_id'].unique()[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f448eba-a9f8-47d2-b6a3-0ba5624d5055",
     "showTitle": false,
     "title": ""
    },
    "id": "b251e072",
    "outputId": "c83b897c-a0bf-41e8-be4a-c5a92a267ae6"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def hybrid_recommendation(user_id, collaborative_model, content_based_data, X_scaled, top_n=10):\n",
    "    # Collaborative Filtering Predictions\n",
    "    collaborative_preds = defaultdict(float)\n",
    "    for item in content_based_data['id'].unique():\n",
    "        prediction = collaborative_model.predict(user_id, item).est\n",
    "        collaborative_preds[item] += prediction\n",
    "\n",
    "    # Content-Based Filtering Recommendations\n",
    "    content_based_recs = content_based_filtering(user_id, content_based_data, X_scaled, top_n=top_n)\n",
    "\n",
    "    # Combine and Sort Recommendations\n",
    "    final_scores = defaultdict(float)\n",
    "    for item in content_based_recs['id']:\n",
    "        final_scores[item] = collaborative_preds[item]  # Assuming the score here\n",
    "\n",
    "    # Sort items based on combined scores\n",
    "    final_recommendations = sorted(final_scores, key=final_scores.get, reverse=True)[:top_n]\n",
    "\n",
    "    return content_based_data[content_based_data['id'].isin(final_recommendations)]\n",
    "\n",
    "# Example usage\n",
    "user_id = 'M758NNC'  # Replace with a real user ID\n",
    "recommendations = hybrid_recommendation(user_id, model, original_data, X_scaled, top_n=10)\n",
    "print(recommendations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "345b9d33-9b52-47b4-aa95-b2368daeb8ab",
     "showTitle": false,
     "title": ""
    },
    "id": "8f58e798"
   },
   "outputs": [],
   "source": [
    "#Evaluating and optimizing your hybrid recommendation system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09f08fec-48a7-4548-8084-129bd7df2d89",
     "showTitle": false,
     "title": ""
    },
    "id": "b171cd90"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_hybrid_recommendations(user_id, collaborative_model, content_based_data, X_scaled, top_n=10):\n",
    "    # Use the hybrid_recommendation function you provided\n",
    "    recommendations = hybrid_recommendation(user_id, collaborative_model, content_based_data, X_scaled, top_n)\n",
    "\n",
    "    # Extract the item IDs from the recommendations\n",
    "    recommended_item_ids = recommendations['id'].tolist()\n",
    "\n",
    "    return recommended_item_ids\n",
    "\n",
    "# Helper function for hybrid recommendations\n",
    "def hybrid_recommendation(user_id, collaborative_model, content_based_data, X_scaled, top_n):\n",
    "    collaborative_preds = defaultdict(float)\n",
    "    for item in content_based_data['id'].unique():\n",
    "        prediction = collaborative_model.predict(user_id, item).est\n",
    "        collaborative_preds[item] += prediction\n",
    "\n",
    "    content_based_recs = content_based_filtering(user_id, content_based_data, X_scaled, top_n=top_n)\n",
    "    final_scores = defaultdict(float)\n",
    "    for item in content_based_recs['id']:\n",
    "        final_scores[item] = collaborative_preds[item]\n",
    "\n",
    "    final_recommendations = sorted(final_scores, key=final_scores.get, reverse=True)[:top_n]\n",
    "    return content_based_data[content_based_data['id'].isin(final_recommendations)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5dd2c8e9-94f5-4cec-90e0-28e3d3569e38",
     "showTitle": false,
     "title": ""
    },
    "id": "60af0ff2",
    "outputId": "dd5d4332-51f6-44c8-be28-b86f7b7cd766"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_actual_items(user_id, test_data):\n",
    "    user_data = test_data[test_data['customer_id'] == user_id]\n",
    "    actual_items = user_data['id'].tolist()\n",
    "    return actual_items\n",
    "\n",
    "def calculate_precision_recall_f1(recommended_items, actual_items, top_n):\n",
    "    top_n_recommended = recommended_items[:top_n]\n",
    "    TP = len(set(top_n_recommended) & set(actual_items))\n",
    "    precision = TP / len(top_n_recommended) if len(top_n_recommended) > 0 else 0\n",
    "    recall = TP / len(actual_items) if len(actual_items) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return precision, recall, f1_score\n",
    "\n",
    "# Load test data and original data for the hybrid recommendation system\n",
    "test_data = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/test_normalized.csv\")\n",
    "original_data = pd.read_csv(\"C:/Users/Joash/Desktop/recommender/train_normalized.csv\")  # or other appropriate dataset\n",
    "\n",
    "# Choose a user ID for evaluation\n",
    "user_id = 'ZCFIWEA'\n",
    "\n",
    "# Get the actual and recommended items\n",
    "actual_items = get_actual_items(user_id, test_data)\n",
    "recommended_items = get_hybrid_recommendations(user_id, model, original_data, X_scaled, top_n=10)\n",
    "\n",
    "# Evaluate the recommendation system\n",
    "precision, recall, f1 = calculate_precision_recall_f1(recommended_items, actual_items, top_n=10)\n",
    "print(f\"Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ddf41dd6-7413-42dc-a851-be333b2dd780",
     "showTitle": false,
     "title": ""
    },
    "id": "774c345a",
    "outputId": "20d96ae0-d239-4a03-a91c-75210382dbcf"
   },
   "outputs": [],
   "source": [
    "# Debugging to check the outputs\n",
    "print(\"Recommended Items:\", recommended_items)\n",
    "print(\"Actual Items:\", actual_items)\n",
    "\n",
    "# If these lists are empty or have no overlap, it will lead to 0 scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f17102ab-1a37-4101-9da0-3443e33c2693",
     "showTitle": false,
     "title": ""
    },
    "id": "09385a51"
   },
   "outputs": [],
   "source": [
    "#Saving the Collaborative Filtering Model and Scaler for Content-Based Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb098a06-15f8-462c-94b3-5ee09cbbae37",
     "showTitle": false,
     "title": ""
    },
    "id": "74dabb44"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Assuming 'scaler' is your StandardScaler instance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Specify your desired path\n",
    "model_path = \"C:\\\\Users\\\\Joash\\\\Desktop\\\\recommender\\\\model.pkl\"\n",
    "scaler_path = \"C:\\\\Users\\\\Joash\\\\Desktop\\\\recommender\\\\scaler.pkl\"\n",
    "\n",
    "# Save the model\n",
    "with open(model_path, \"wb\") as file:\n",
    "    pickle.dump(model, file)\n",
    "\n",
    "# Save the scaler\n",
    "with open(scaler_path, \"wb\") as file:\n",
    "    pickle.dump(scaler, file)\n",
    "\n",
    "np.save(\"X_scaled.npy\", X_scaled)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "local_model",
   "widgets": {}
  },
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (surprise_env)",
   "language": "python",
   "name": "surprise_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
